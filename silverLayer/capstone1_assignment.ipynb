{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e2ba3f",
   "metadata": {},
   "source": [
    "## Load the SentenceTransformer and test the embedding is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3524f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embeddingMModel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# def embed_text(text):\n",
    "#     return embeddingModel.encode(text)\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     sample_text = \"This is a sample text for embedding.\"\n",
    "#     embedding = embed_text(sample_text)\n",
    "#     print(f\"Embedding for the sample text: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e4f49",
   "metadata": {},
   "source": [
    "### Load Document and spilt into chunks and then embedding the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99c4aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 899 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 122\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the pdf document\n",
    "loader = PyPDFLoader(\"dataset/AI Agents guidebook.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc839b",
   "metadata": {},
   "source": [
    "### Embed the chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea7f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for first chunk: [-0.05577217 -0.04005972  0.02752373  0.03279826  0.05366068]\n",
      "Total number of encoded chunks: 122\n",
      "Length of each encoded chunk: 384\n",
      "Embedding Model Shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "encoded_chunks = [embeddingMModel.encode(chunk.page_content) for chunk in chunks]\n",
    "print(f\"Embedding for first chunk: {encoded_chunks[0][:5]}\")\n",
    "print(f'Total number of encoded chunks: {len(encoded_chunks)}')\n",
    "print(f'Length of each encoded chunk: {len(encoded_chunks[1])}')\n",
    "print(f'Embedding Model Shape: {encoded_chunks[0].shape}')\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"Number of chunks created: {len(chunks)}\")\n",
    "#     print(f\"Embedding for first chunk: {encoded_chunks[0][:5]}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f759b9c",
   "metadata": {},
   "source": [
    "### Save the embedding in Chromdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "870dc7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Insert of existing embedding ID: 7\n",
      "Add of existing embedding ID: 8\n",
      "Insert of existing embedding ID: 8\n",
      "Add of existing embedding ID: 9\n",
      "Insert of existing embedding ID: 9\n",
      "Add of existing embedding ID: 10\n",
      "Insert of existing embedding ID: 10\n",
      "Add of existing embedding ID: 11\n",
      "Insert of existing embedding ID: 11\n",
      "Add of existing embedding ID: 12\n",
      "Insert of existing embedding ID: 12\n",
      "Add of existing embedding ID: 13\n",
      "Insert of existing embedding ID: 13\n",
      "Add of existing embedding ID: 14\n",
      "Insert of existing embedding ID: 14\n",
      "Add of existing embedding ID: 15\n",
      "Insert of existing embedding ID: 15\n",
      "Add of existing embedding ID: 16\n",
      "Insert of existing embedding ID: 16\n",
      "Add of existing embedding ID: 17\n",
      "Insert of existing embedding ID: 17\n",
      "Add of existing embedding ID: 18\n",
      "Insert of existing embedding ID: 18\n",
      "Add of existing embedding ID: 19\n",
      "Insert of existing embedding ID: 19\n",
      "Add of existing embedding ID: 20\n",
      "Insert of existing embedding ID: 20\n",
      "Add of existing embedding ID: 21\n",
      "Insert of existing embedding ID: 21\n",
      "Add of existing embedding ID: 22\n",
      "Insert of existing embedding ID: 22\n",
      "Add of existing embedding ID: 23\n",
      "Insert of existing embedding ID: 23\n",
      "Add of existing embedding ID: 24\n",
      "Insert of existing embedding ID: 24\n",
      "Add of existing embedding ID: 25\n",
      "Insert of existing embedding ID: 25\n",
      "Add of existing embedding ID: 26\n",
      "Insert of existing embedding ID: 26\n",
      "Add of existing embedding ID: 27\n",
      "Insert of existing embedding ID: 27\n",
      "Add of existing embedding ID: 28\n",
      "Insert of existing embedding ID: 28\n",
      "Add of existing embedding ID: 29\n",
      "Insert of existing embedding ID: 29\n",
      "Add of existing embedding ID: 30\n",
      "Insert of existing embedding ID: 30\n",
      "Add of existing embedding ID: 31\n",
      "Insert of existing embedding ID: 31\n",
      "Add of existing embedding ID: 32\n",
      "Insert of existing embedding ID: 32\n",
      "Add of existing embedding ID: 33\n",
      "Insert of existing embedding ID: 33\n",
      "Add of existing embedding ID: 34\n",
      "Insert of existing embedding ID: 34\n",
      "Add of existing embedding ID: 35\n",
      "Insert of existing embedding ID: 35\n",
      "Add of existing embedding ID: 36\n",
      "Insert of existing embedding ID: 36\n",
      "Add of existing embedding ID: 37\n",
      "Insert of existing embedding ID: 37\n",
      "Add of existing embedding ID: 38\n",
      "Insert of existing embedding ID: 38\n",
      "Add of existing embedding ID: 39\n",
      "Insert of existing embedding ID: 39\n",
      "Add of existing embedding ID: 40\n",
      "Insert of existing embedding ID: 40\n",
      "Add of existing embedding ID: 41\n",
      "Insert of existing embedding ID: 41\n",
      "Add of existing embedding ID: 42\n",
      "Insert of existing embedding ID: 42\n",
      "Add of existing embedding ID: 43\n",
      "Insert of existing embedding ID: 43\n",
      "Add of existing embedding ID: 44\n",
      "Insert of existing embedding ID: 44\n",
      "Add of existing embedding ID: 45\n",
      "Insert of existing embedding ID: 45\n",
      "Add of existing embedding ID: 46\n",
      "Insert of existing embedding ID: 46\n",
      "Add of existing embedding ID: 47\n",
      "Insert of existing embedding ID: 47\n",
      "Add of existing embedding ID: 48\n",
      "Insert of existing embedding ID: 48\n",
      "Add of existing embedding ID: 49\n",
      "Insert of existing embedding ID: 49\n",
      "Add of existing embedding ID: 50\n",
      "Insert of existing embedding ID: 50\n",
      "Add of existing embedding ID: 51\n",
      "Insert of existing embedding ID: 51\n",
      "Add of existing embedding ID: 52\n",
      "Insert of existing embedding ID: 52\n",
      "Add of existing embedding ID: 53\n",
      "Insert of existing embedding ID: 53\n",
      "Add of existing embedding ID: 54\n",
      "Insert of existing embedding ID: 54\n",
      "Add of existing embedding ID: 55\n",
      "Insert of existing embedding ID: 55\n",
      "Add of existing embedding ID: 56\n",
      "Insert of existing embedding ID: 56\n",
      "Add of existing embedding ID: 57\n",
      "Insert of existing embedding ID: 57\n",
      "Add of existing embedding ID: 58\n",
      "Insert of existing embedding ID: 58\n",
      "Add of existing embedding ID: 59\n",
      "Insert of existing embedding ID: 59\n",
      "Add of existing embedding ID: 60\n",
      "Insert of existing embedding ID: 60\n",
      "Add of existing embedding ID: 61\n",
      "Insert of existing embedding ID: 61\n",
      "Add of existing embedding ID: 62\n",
      "Insert of existing embedding ID: 62\n",
      "Add of existing embedding ID: 63\n",
      "Insert of existing embedding ID: 63\n",
      "Add of existing embedding ID: 64\n",
      "Insert of existing embedding ID: 64\n",
      "Add of existing embedding ID: 65\n",
      "Insert of existing embedding ID: 65\n",
      "Add of existing embedding ID: 66\n",
      "Insert of existing embedding ID: 66\n",
      "Add of existing embedding ID: 67\n",
      "Insert of existing embedding ID: 67\n",
      "Add of existing embedding ID: 68\n",
      "Insert of existing embedding ID: 68\n",
      "Add of existing embedding ID: 69\n",
      "Insert of existing embedding ID: 69\n",
      "Add of existing embedding ID: 70\n",
      "Insert of existing embedding ID: 70\n",
      "Add of existing embedding ID: 71\n",
      "Insert of existing embedding ID: 71\n",
      "Add of existing embedding ID: 72\n",
      "Insert of existing embedding ID: 72\n",
      "Add of existing embedding ID: 73\n",
      "Insert of existing embedding ID: 73\n",
      "Add of existing embedding ID: 74\n",
      "Insert of existing embedding ID: 74\n",
      "Add of existing embedding ID: 75\n",
      "Insert of existing embedding ID: 75\n",
      "Add of existing embedding ID: 76\n",
      "Insert of existing embedding ID: 76\n",
      "Add of existing embedding ID: 77\n",
      "Insert of existing embedding ID: 77\n",
      "Add of existing embedding ID: 78\n",
      "Insert of existing embedding ID: 78\n",
      "Add of existing embedding ID: 79\n",
      "Insert of existing embedding ID: 79\n",
      "Add of existing embedding ID: 80\n",
      "Insert of existing embedding ID: 80\n",
      "Add of existing embedding ID: 81\n",
      "Insert of existing embedding ID: 81\n",
      "Add of existing embedding ID: 82\n",
      "Insert of existing embedding ID: 82\n",
      "Add of existing embedding ID: 83\n",
      "Insert of existing embedding ID: 83\n",
      "Add of existing embedding ID: 84\n",
      "Insert of existing embedding ID: 84\n",
      "Add of existing embedding ID: 85\n",
      "Insert of existing embedding ID: 85\n",
      "Add of existing embedding ID: 86\n",
      "Insert of existing embedding ID: 86\n",
      "Add of existing embedding ID: 87\n",
      "Insert of existing embedding ID: 87\n",
      "Add of existing embedding ID: 88\n",
      "Insert of existing embedding ID: 88\n",
      "Add of existing embedding ID: 89\n",
      "Insert of existing embedding ID: 89\n",
      "Add of existing embedding ID: 90\n",
      "Insert of existing embedding ID: 90\n",
      "Add of existing embedding ID: 91\n",
      "Insert of existing embedding ID: 91\n",
      "Add of existing embedding ID: 92\n",
      "Insert of existing embedding ID: 92\n",
      "Add of existing embedding ID: 93\n",
      "Insert of existing embedding ID: 93\n",
      "Add of existing embedding ID: 94\n",
      "Insert of existing embedding ID: 94\n",
      "Add of existing embedding ID: 95\n",
      "Insert of existing embedding ID: 95\n",
      "Add of existing embedding ID: 96\n",
      "Insert of existing embedding ID: 96\n",
      "Add of existing embedding ID: 97\n",
      "Insert of existing embedding ID: 97\n",
      "Add of existing embedding ID: 98\n",
      "Insert of existing embedding ID: 98\n",
      "Add of existing embedding ID: 99\n",
      "Insert of existing embedding ID: 99\n",
      "Add of existing embedding ID: 100\n",
      "Insert of existing embedding ID: 100\n",
      "Add of existing embedding ID: 101\n",
      "Insert of existing embedding ID: 101\n",
      "Add of existing embedding ID: 102\n",
      "Insert of existing embedding ID: 102\n",
      "Add of existing embedding ID: 103\n",
      "Insert of existing embedding ID: 103\n",
      "Add of existing embedding ID: 104\n",
      "Insert of existing embedding ID: 104\n",
      "Add of existing embedding ID: 105\n",
      "Insert of existing embedding ID: 105\n",
      "Add of existing embedding ID: 106\n",
      "Insert of existing embedding ID: 106\n",
      "Add of existing embedding ID: 107\n",
      "Insert of existing embedding ID: 107\n",
      "Add of existing embedding ID: 108\n",
      "Insert of existing embedding ID: 108\n",
      "Add of existing embedding ID: 109\n",
      "Insert of existing embedding ID: 109\n",
      "Add of existing embedding ID: 110\n",
      "Insert of existing embedding ID: 110\n",
      "Add of existing embedding ID: 111\n",
      "Insert of existing embedding ID: 111\n",
      "Add of existing embedding ID: 112\n",
      "Insert of existing embedding ID: 112\n",
      "Add of existing embedding ID: 113\n",
      "Insert of existing embedding ID: 113\n",
      "Add of existing embedding ID: 114\n",
      "Insert of existing embedding ID: 114\n",
      "Add of existing embedding ID: 115\n",
      "Insert of existing embedding ID: 115\n",
      "Add of existing embedding ID: 116\n",
      "Insert of existing embedding ID: 116\n",
      "Add of existing embedding ID: 117\n",
      "Insert of existing embedding ID: 117\n",
      "Add of existing embedding ID: 118\n",
      "Insert of existing embedding ID: 118\n",
      "Add of existing embedding ID: 119\n",
      "Insert of existing embedding ID: 119\n",
      "Add of existing embedding ID: 120\n",
      "Insert of existing embedding ID: 120\n",
      "Add of existing embedding ID: 121\n",
      "Insert of existing embedding ID: 121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to ChromaDB collection 'ai_agents_guidebook'. 131 items added.\n",
      "Number of chunks created: Collection(name=ai_agents_guidebook)\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create a ChromaDB client\n",
    "client = chromadb.Client()\n",
    "# Create a collection to store the embeddings\n",
    "collection = client.get_or_create_collection(name=\"ai_agents_guidebook\")\n",
    "# Add the chunks , their embeddings and metadata to the collection\n",
    "for i, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content],\n",
    "        embeddings=[encoded_chunks[i]],\n",
    "        ids=[str(i)],\n",
    "        metadatas=[{\"source\": \"AI Agents guidebook\", \"chunk_index\": i}]\n",
    "    )\n",
    "print(f\"Embeddings saved to ChromaDB collection 'ai_agents_guidebook'. {collection.count()} items added.\")\n",
    "print(f\"Number of chunks created: {collection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8ffc5",
   "metadata": {},
   "source": [
    "### Query and retrieve the vector db here. it is chromadb... Testing done before using LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41fc4b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç TESTING SEMANTIC SEARCH\n",
      "============================================================\n",
      "Results for query: What is MCP Server\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.8619 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Finally, once we have all the a...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.9035 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#8) Integrate MCP server with...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.9076 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "#7) Integrate MCP server with C...\n",
      "Results for query: What is Kayak Tool?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.5971 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Kayak tool \n",
      "A custom Kaya...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.6021 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Kayak tool \n",
      "A custom Kaya...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.0708 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "#5) Browserbase Tool \n",
      "The Ô¨Çight...\n",
      "Results for query: Can i work from home today?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4353 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "Streamlit UI \n",
      "To make this ac...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4730 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Streamlit UI \n",
      "To make this acce...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.5109 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Scheduling \n",
      "After the con...\n",
      "Results for query: How to use AutoGPT?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.3597 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Now, we deÔ¨Åne the CurrencyConve...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.3844 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Finally, we tie it all together...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4952 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "You would also need an API ke...\n"
     ]
    }
   ],
   "source": [
    "# create a method which will take query and return the relevant chunks\n",
    "def query_chromadb(query, top_k=3):\n",
    "    # Embed the query using the same embedding model\n",
    "    query_embedding = embeddingMModel.encode(query)\n",
    "    # Query the collection for similar embeddings\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "       \n",
    "# Test the method using Query list before using LLM\n",
    "query_list = [\n",
    "    \"What is MCP Server\",\n",
    "    \"What is Kayak Tool?\",\n",
    "    \"Can i work from home today?\",\n",
    "    \"How to use AutoGPT?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Example usage with the query list\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç TESTING SEMANTIC SEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    for query in query_list:\n",
    "        print(f\"Results for query: {query}\")\n",
    "        results = query_chromadb(query, top_k=3)\n",
    "        for i, (doc, score, metadata) in enumerate(zip(results['documents'][0], results['distances'][0], results['metadatas'][0])):\n",
    "            print(f\"\\n  Result {i+1}:\")\n",
    "            print(f\"  üìÑ Topic: {metadata['source']}\")\n",
    "            print(f\"  üìè Distance: {score:.4f} (lower = more similar)\")\n",
    "            print(f\"  üìñ Text: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb73a",
   "metadata": {},
   "source": [
    "## Initlaise the SQLite for storing the response generated by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55f2939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4f5dd",
   "metadata": {},
   "source": [
    "### Create the Prompt with User Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d11039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "\n",
      "You are an AI assistant helping users find information in a document. Use the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know.\n",
      "\n",
      "\n",
      "Context 1:\n",
      "DailyDoseofDS.com \n",
      "Finally, once we have all the agents and tools deÔ¨Åned we set up and kickoÔ¨Ä our \n",
      "deep researcher crew. \n",
      " \n",
      "#7) Create MCP Server \n",
      "Now, we'll encapsulate our deep research team within an MCP tool. With just a \n",
      "few lines of code, our MCP server will be ready. \n",
      "Let's see how to connect it with Cursor. \n",
      "79\n",
      "\n",
      "Context 2:\n",
      "DailyDoseofDS.com \n",
      " \n",
      "#8) Integrate MCP server with Cursor \n",
      "Go to: File ‚Üí Preferences ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new global MCP \n",
      "server \n",
      "In the JSON Ô¨Åle, add what's shown below  \n",
      "80\n",
      "\n",
      "Context 3:\n",
      "DailyDoseofDS.com \n",
      "#5) Setup Crew and KickoÔ¨Ä \n",
      "We set up and kick oÔ¨Ä our Ô¨Ånancial analysis crew to get the result shown below! \n",
      " \n",
      "#6) Create MCP Server \n",
      "Now, we encapsulate our Ô¨Ånancial analyst within an MCP tool and add two more \n",
      "tools to enhance the user experience. \n",
      "‚óè save_code -> Saves generated code to local directory \n",
      "‚óè run_code_and_show_plot -> Executes the code and generates a plot \n",
      " \n",
      "57\n",
      "\n",
      "Question: What is MCP Server?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "system_prompt_context = \"\"\"\n",
    "You are an AI assistant helping users find information in a document. Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "\"\"\"\n",
    "def create_prompt(user_query, context_chunks):\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "    prompt = f\"{system_prompt_context}\\n\\n{context_text}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What is MCP Server?\"\n",
    "    results = query_chromadb(user_query, top_k=3)\n",
    "    # print(f'Results Retrieved: {results}')\n",
    "    context_chunks = results['documents'][0]\n",
    "    # print(f'Context Chunks Retrieved: {len(context_chunks)}')\n",
    "    # print(f'First Context Chunk: {context_chunks[2]}...\\n\\n')\n",
    "    prompt = create_prompt(user_query, context_chunks)\n",
    "    print(\"Generated Prompt:\")\n",
    "    print(prompt)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991103d",
   "metadata": {},
   "source": [
    "### Call the Groq and OpenAI  LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f766b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Initialize LLMs\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "google_llm = None\n",
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, google_api_key=google_api_key)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a8ad9",
   "metadata": {},
   "source": [
    "## Pass the query and context to create a response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05e2014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Google LLM Response: The context describes a \"Multi-agent Deep Researcher\" that functions similarly to ChatGPT's deep research feature, providing detailed insights on any topic. It can be built as a local alternative using a specific tech stack. The workflow involves a user submitting a query, a web search agent performing a deep web search, a research analyst verifying and deduplicating the results, and a technical writer crafting a coherent response with citations. This system utilizes multiple agents working together, with one agent for research and another for analysis and writing.\n",
      "\n",
      "\n",
      " Groq LLM Response: Based on the provided context, a multi-agent team researcher appears to be a system where multiple agents work together to provide detailed insights on a topic. \n",
      "\n",
      "From Context 2, it seems that such a system can be built using a tech stack that includes a platform for deep web research, multi-agent orchestration, and a local server for hosting the research feature. The workflow involves a user submitting a query, a web search agent conducting a deep web search, a research analyst verifying and deduplicating results, and a technical writer crafting a coherent response with citations.\n",
      "\n",
      "Additionally, Context 1 mentions that multi-agent systems work best when agents collaborate and exchange feedback, and provides an example of an AI-powered financial analysis system where multiple agents work together to gather data, assess risk, build strategy, and write a report.\n",
      "\n",
      "Context 3 also mentions a Crew setup with multiple agents, including an Analysis Agent and a Writer Agent, which could be part of a multi-agent team researcher.\n",
      "\n",
      "However, I don't have more specific information about the \"multi agent team researcher\" beyond what is provided in these contexts.\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(user_query, context_chunks):\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "    prompt = f\"{system_prompt_context}\\n\\n{context_text}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def generate_openai_response(prompt):\n",
    "    response = openai_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Open AI Response: {response.content} *80')\n",
    "    return response.content\n",
    "\n",
    "def generate_google_response(prompt):\n",
    "    response = google_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Google LLM Response: {response.content}')\n",
    "    return response.content\n",
    "\n",
    "def generate_groq_response(prompt):\n",
    "    response = groq_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Groq LLM Response: {response.content}')\n",
    "    return response.content\n",
    "\n",
    "\n",
    "\n",
    "user_query = \"Tell me more about multi agent team researcher?\"\n",
    "results = query_chromadb(user_query, top_k=3)\n",
    "\n",
    "# print(f'Results Retrieved: {results}')\n",
    "context_chunks = results['documents'][0]\n",
    "# print(f'Context Chunks Retrieved: {len(context_chunks)}')\n",
    "# print(f'First Context Chunk: {context_chunks[2]}...\\n\\n')\n",
    "prompt = create_prompt(user_query, context_chunks)\n",
    "# print(\"Generated Prompt:\")\n",
    "# print(prompt)  \n",
    "\n",
    "# Generate responses from both LLMs\n",
    "gemini_llmresponse = generate_google_response(prompt)\n",
    "groq_response = generate_groq_response(prompt)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a199d01",
   "metadata": {},
   "source": [
    "## Save the final Syntentized Response in sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d64e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Synthesized Final Response:\n",
      "A \"Multi-agent Team Researcher\" is a sophisticated system designed to provide in-depth insights on any given topic, functioning analogously to advanced deep research features found in platforms like ChatGPT. This system can be constructed as a local alternative, leveraging a specific technology stack.\n",
      "\n",
      "The core of this researcher lies in its multi-agent architecture, where distinct AI agents collaborate to achieve a common goal. The typical workflow begins with a user submitting a query. This query is then handled by a **web search agent** responsible for conducting a comprehensive, deep web search to gather relevant information. Following the search, a **research analyst agent** takes over to meticulously verify the gathered data, deduplicate redundant findings, and ensure the accuracy and reliability of the information. Finally, a **technical writer agent** synthesizes the verified research into a coherent, well-structured response, complete with proper citations.\n",
      "\n",
      "This collaborative approach is a hallmark of effective multi-agent systems. As highlighted, these systems thrive when agents actively exchange feedback and work in tandem. An illustrative example of this principle is an AI-powered financial analysis system, where multiple agents might collectively gather data, assess risks, formulate strategies, and ultimately produce a detailed report.\n",
      "\n",
      "The underlying technology for building such a system typically involves a platform capable of deep web research, a robust multi-agent orchestration framework, and a local server for hosting the entire feature. Within this framework, specific agent roles, such as an \"Analysis Agent\" and a \"Writer Agent,\" as seen in a \"Crew\" setup, are integral components of the multi-agent team researcher.\n",
      "\n",
      "\n",
      " embedd query is 0.007471877615898848\n",
      "Fetched vector binding 0.007471877615898848\n",
      "\n",
      "\n",
      " Closing Database connection\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "\n",
    "synthesis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Synthesize these two answers into one comprehensive response:\n",
    "\n",
    "Answer from Gemini LLM:\n",
    "{response1}\n",
    "\n",
    "Answer from Groq Llama:\n",
    "{response2}\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Combine the best insights from both answers\n",
    "2. Avoid redundancy\n",
    "3. Provide a complete, well-organized answer\n",
    "4. Maintain accuracy and clarity\n",
    "\n",
    "Provide a single synthesized answer that is better and more complete than both.\n",
    "\"\"\")\n",
    "\n",
    "chain_synthesis = synthesis_prompt | google_llm | StrOutputParser()\n",
    "final_response = chain_synthesis.invoke({\n",
    "    \"response1\": gemini_llmresponse,\n",
    "    \"response2\": groq_response,\n",
    "    \"question\": user_query,\n",
    "\n",
    "})\n",
    "\n",
    "print(f\"‚úì Synthesized Final Response:\\n{final_response}\")\n",
    "\n",
    "# create a database connection\n",
    "sqlite_db_conn = sqlite3.connect(\"documents.db\")\n",
    "\n",
    "# Save it to SQLite\n",
    "cursor = sqlite_db_conn.cursor()\n",
    "\n",
    "# 2. Create a table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS llm_response (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    user_query TEXT,\n",
    "    vector_embedding BLOB,\n",
    "    llmresponse TEXT\n",
    ")\n",
    "\"\"\")\n",
    "# convert the query_vectorbinding into a BLOB using numpy array\n",
    "queryEmbedding = embeddingMModel.encode(user_query)\n",
    "print(f'\\n\\n embedd query is {queryEmbedding[0]}')\n",
    "\n",
    "embeddingBlob = np.array(queryEmbedding,dtype=np.float32).tobytes()\n",
    "\n",
    "\n",
    "# 3. Insert a record\n",
    "cursor.execute(\"\"\"\n",
    "INSERT INTO llm_response (user_query, vector_embedding, llmresponse)\n",
    "VALUES (?, ?, ?)\n",
    "\"\"\", (user_query, embeddingBlob, final_response))\n",
    "\n",
    "# 4. Commit changes\n",
    "sqlite_db_conn.commit()\n",
    "\n",
    "# 5. Verify vector embedding by decode\n",
    "cursor.execute(\n",
    "    \"SELECT vector_embedding FROM llm_response WHERE user_query = ?\",\n",
    "    (user_query,)\n",
    ")\n",
    "blob = cursor.fetchone()[0]\n",
    "\n",
    "vectorFetchedVal = np.frombuffer(blob, dtype=np.float32)\n",
    "print(f'Fetched vector binding {vectorFetchedVal[0]}')\n",
    "\n",
    "\n",
    "print(\"\\n\\n Closing Database connection\")\n",
    "sqlite_db_conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989de05",
   "metadata": {},
   "source": [
    "## Shorten the summary to 280 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "300cfc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úì AI-Generated Summary (280 characters max):\n",
      "================================================================================\n",
      "A Multi-agent Team Researcher is a local AI system for deep topic insights. It uses specialized agents: a web search agent gathers info, a research analyst verifies it, and a technical writer crafts a cited response. This collaborative approach ensures accurate, structured res...\n",
      "\n",
      "Original length: 1764 characters\n",
      "Summary length: 280 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a summarization prompt to condense the response to 280 characters\n",
    "summarization_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize the following text in exactly 280 characters or less. \n",
    "Make it concise, clear, and informative. Preserve the key message.\n",
    "\n",
    "Text to summarize:\n",
    "{text}\n",
    "\n",
    "Important: Your summary must be exactly 280 characters or less (including spaces and punctuation).\n",
    "Respond with ONLY the summarized text, no additional explanation.\n",
    "\"\"\")\n",
    "\n",
    "# Use Google LLM to create the 280-character summary\n",
    "chain_summarize = summarization_prompt | google_llm | StrOutputParser()\n",
    "\n",
    "shortened_response = chain_summarize.invoke({\n",
    "    \"text\": final_response\n",
    "})\n",
    "\n",
    "# Ensure the response doesn't exceed 280 characters (fallback truncation)\n",
    "if len(shortened_response) > 280:\n",
    "    shortened_response = shortened_response[:277] + \"...\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì AI-Generated Summary (280 characters max):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{shortened_response}\")\n",
    "print(f\"\\nOriginal length: {len(final_response)} characters\")\n",
    "print(f\"Summary length: {len(shortened_response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cd141",
   "metadata": {},
   "source": [
    "### Send the summarized summary as a email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c88df4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: GMAIL_SENDER_EMAIL or GMAIL_APP_PASSWORD not found in .env file\n",
      "Please add the following to your .env file:\n",
      "  GMAIL_SENDER_EMAIL=your_email@gmail.com\n",
      "  GMAIL_APP_PASSWORD=your_app_password\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sendgrid import SendGridAPIClient\n",
    "from sendgrid.helpers.mail import Mail\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def send_email_with_sendgrid(summarized_text, recipient_email=\"joyshanker78@gmail.com\"):\n",
    "    \"\"\"\n",
    "    Send the summarized response via email using Gmail SMTP\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get email credentials from environment variables\n",
    "    sender_email = \"joyshanker78@gmail.com\" #recipient_email #os.getenv(\"GMAIL_SENDER_EMAIL\")\n",
    "    #recipient_email = sender_email\n",
    "    app_password = \"Check123$\"\n",
    "    \n",
    "    if not sender_email or not app_password:\n",
    "        print(\"‚ùå Error: GMAIL_SENDER_EMAIL or GMAIL_APP_PASSWORD not found in .env file\")\n",
    "        print(\"Please add the following to your .env file:\")\n",
    "        print(\"  GMAIL_SENDER_EMAIL=your_email@gmail.com\")\n",
    "        print(\"  GMAIL_APP_PASSWORD=your_app_password\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create message\n",
    "        message = MIMEMultipart(\"alternative\")\n",
    "        message[\"Subject\"] = \"AI-Generated Summary: Multi-Agent Team Researcher\"\n",
    "        message[\"From\"] = sender_email\n",
    "        message[\"To\"] = recipient_email\n",
    "        \n",
    "        # Create HTML email body\n",
    "        html_body = f\"\"\"\n",
    "        <html>\n",
    "          <body style=\"font-family: Arial, sans-serif; line-height: 1.6; color: #333;\">\n",
    "            <div style=\"max-width: 600px; margin: 0 auto; padding: 20px; border: 1px solid #ddd; border-radius: 5px;\">\n",
    "              <h2 style=\"color: #4CAF50;\">‚úì AI-Generated Summary</h2>\n",
    "              \n",
    "              <p><strong>Topic:</strong> Multi-Agent Team Researcher</p>\n",
    "              \n",
    "              <div style=\"background-color: #f5f5f5; padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;\">\n",
    "                <p>{summarized_text}</p>\n",
    "              </div>\n",
    "              \n",
    "              <p><strong>Summary Statistics:</strong></p>\n",
    "              <ul>\n",
    "                <li>Character Length: {len(summarized_text)}/280</li>\n",
    "                <li>Generated using: Google Generative AI (Gemini)</li>\n",
    "                <li>Timestamp: {pd.Timestamp.now()}</li>\n",
    "              </ul>\n",
    "              \n",
    "              <hr>\n",
    "              <p style=\"font-size: 12px; color: #666;\">This is an automated email from AI Capstone Assignment. Please do not reply to this email.</p>\n",
    "            </div>\n",
    "          </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create plain text version\n",
    "        text_body = f\"\"\"\n",
    "        AI-Generated Summary\n",
    "        ====================\n",
    "        \n",
    "        Topic: Multi-Agent Team Researcher\n",
    "        \n",
    "        {summarized_text}\n",
    "        \n",
    "        Character Length: {len(summarized_text)}/280\n",
    "        Generated using: Google Generative AI (Gemini)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Attach both versions\n",
    "        message.attach(MIMEText(text_body, \"plain\"))\n",
    "        message.attach(MIMEText(html_body, \"html\"))\n",
    "        \n",
    "        # Send email via Gmail SMTP\n",
    "        print(\"\\nüìß Sending email...\")\n",
    "        server = smtplib.SMTP_SSL(\"smtp.gmail.com\", 465)\n",
    "        server.login(sender_email, app_password)\n",
    "        server.sendmail(sender_email, recipient_email, message.as_string())\n",
    "        server.quit()\n",
    "        \n",
    "        print(f\"‚úì Email sent successfully to {recipient_email}\")\n",
    "        return True\n",
    "        \n",
    "    except smtplib.SMTPAuthenticationError:\n",
    "        print(\"‚ùå Error: Gmail authentication failed. Check your credentials.\")\n",
    "        return False\n",
    "    except smtplib.SMTPException as e:\n",
    "        print(f\"‚ùå SMTP error occurred: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Send the summarized response\n",
    "send_email_with_summary(shortened_response,\"joyshanker78@gmail.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
