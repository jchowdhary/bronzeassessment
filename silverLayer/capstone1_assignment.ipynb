{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e2ba3f",
   "metadata": {},
   "source": [
    "## Load the SentenceTransformer and test the embedding is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3524f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embeddingMModel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# def embed_text(text):\n",
    "#     return embeddingModel.encode(text)\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     sample_text = \"This is a sample text for embedding.\"\n",
    "#     embedding = embed_text(sample_text)\n",
    "#     print(f\"Embedding for the sample text: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057e4f49",
   "metadata": {},
   "source": [
    "### Load Document and spilt into chunks and then embedding the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c4aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 899 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 122\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# load the pdf document\n",
    "loader = PyPDFLoader(\"dataset/AI Agents guidebook.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=300)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of chunks created: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc839b",
   "metadata": {},
   "source": [
    "### Embed the chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ea7f4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for first chunk: [-0.05577217 -0.04005972  0.02752373  0.03279826  0.05366068]\n",
      "Total number of encoded chunks: 122\n",
      "Length of each encoded chunk: 384\n",
      "Embedding Model Shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "encoded_chunks = [embeddingMModel.encode(chunk.page_content) for chunk in chunks]\n",
    "print(f\"Embedding for first chunk: {encoded_chunks[0][:5]}\")\n",
    "print(f'Total number of encoded chunks: {len(encoded_chunks)}')\n",
    "print(f'Length of each encoded chunk: {len(encoded_chunks[1])}')\n",
    "print(f'Embedding Model Shape: {encoded_chunks[0].shape}')\n",
    "# Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(f\"Number of chunks created: {len(chunks)}\")\n",
    "#     print(f\"Embedding for first chunk: {encoded_chunks[0][:5]}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f759b9c",
   "metadata": {},
   "source": [
    "### Save the embedding in Chromdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870dc7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to ChromaDB collection 'ai_agents_guidebook'. 122 items added.\n",
      "Number of chunks created: Collection(name=ai_agents_guidebook)\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create a ChromaDB client\n",
    "client = chromadb.Client()\n",
    "# Create a collection to store the embeddings\n",
    "collection = client.get_or_create_collection(name=\"ai_agents_guidebook\")\n",
    "# Add the chunks , their embeddings and metadata to the collection\n",
    "for i, chunk in enumerate(chunks):\n",
    "    collection.add(\n",
    "        documents=[chunk.page_content],\n",
    "        embeddings=[encoded_chunks[i]],\n",
    "        ids=[str(i)],\n",
    "        metadatas=[{\"source\": \"AI Agents guidebook\", \"chunk_index\": i}]\n",
    "    )\n",
    "print(f\"Embeddings saved to ChromaDB collection 'ai_agents_guidebook'. {collection.count()} items added.\")\n",
    "print(f\"Number of chunks created: {collection}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb8ffc5",
   "metadata": {},
   "source": [
    "### Query and retrieve the vector db here. it is chromadb... Testing done before using LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fc4b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç TESTING SEMANTIC SEARCH\n",
      "============================================================\n",
      "Results for query: What is MCP Server\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.8619 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Finally, once we have all the a...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.9035 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#8) Integrate MCP server with...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.9076 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "#7) Integrate MCP server with C...\n",
      "Results for query: What is Kayak Tool?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.5971 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Kayak tool \n",
      "A custom Kaya...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 0.6021 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Kayak tool \n",
      "A custom Kaya...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.0708 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "#5) Browserbase Tool \n",
      "The Ô¨Çight...\n",
      "Results for query: Can i work from home today?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4353 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "Streamlit UI \n",
      "To make this ac...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4730 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Streamlit UI \n",
      "To make this acce...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.5109 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "#4) Scheduling \n",
      "After the con...\n",
      "Results for query: How to use AutoGPT?\n",
      "\n",
      "  Result 1:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.3597 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Now, we deÔ¨Åne the CurrencyConve...\n",
      "\n",
      "  Result 2:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.3844 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      "Finally, we tie it all together...\n",
      "\n",
      "  Result 3:\n",
      "  üìÑ Topic: AI Agents guidebook\n",
      "  üìè Distance: 1.4952 (lower = more similar)\n",
      "  üìñ Text: DailyDoseofDS.com \n",
      " \n",
      "You would also need an API ke...\n"
     ]
    }
   ],
   "source": [
    "# create a method which will take query and return the relevant chunks\n",
    "def query_chromadb(query, top_k=3):\n",
    "    # Embed the query using the same embedding model\n",
    "    query_embedding = embeddingMModel.encode(query)\n",
    "    # Query the collection for similar embeddings\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results\n",
    "\n",
    "       \n",
    "# Test the method using Query list before using LLM\n",
    "query_list = [\n",
    "    \"What is MCP Server\",\n",
    "    \"What is Kayak Tool?\",\n",
    "    \"Can i work from home today?\",\n",
    "    \"How to use AutoGPT?\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Example usage with the query list\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç TESTING SEMANTIC SEARCH\")\n",
    "    print(\"=\" * 60)\n",
    "    for query in query_list:\n",
    "        print(f\"Results for query: {query}\")\n",
    "        results = query_chromadb(query, top_k=3)\n",
    "        for i, (doc, score, metadata) in enumerate(zip(results['documents'][0], results['distances'][0], results['metadatas'][0])):\n",
    "            print(f\"\\n  Result {i+1}:\")\n",
    "            print(f\"  üìÑ Topic: {metadata['source']}\")\n",
    "            print(f\"  üìè Distance: {score:.4f} (lower = more similar)\")\n",
    "            print(f\"  üìñ Text: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcb73a",
   "metadata": {},
   "source": [
    "## Initlaise the SQLite for storing the response generated by LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55f2939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a4f5dd",
   "metadata": {},
   "source": [
    "### Create the Prompt with User Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d11039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "\n",
      "You are an AI assistant helping users find information in a document. Use the following pieces of context to answer the question at the end. \n",
      "If you don't know the answer, just say that you don't know.\n",
      "\n",
      "\n",
      "Context 1:\n",
      "DailyDoseofDS.com \n",
      "Finally, once we have all the agents and tools deÔ¨Åned we set up and kickoÔ¨Ä our \n",
      "deep researcher crew. \n",
      " \n",
      "#7) Create MCP Server \n",
      "Now, we'll encapsulate our deep research team within an MCP tool. With just a \n",
      "few lines of code, our MCP server will be ready. \n",
      "Let's see how to connect it with Cursor. \n",
      "79\n",
      "\n",
      "Context 2:\n",
      "DailyDoseofDS.com \n",
      " \n",
      "#8) Integrate MCP server with Cursor \n",
      "Go to: File ‚Üí Preferences ‚Üí Cursor Settings ‚Üí MCP ‚Üí Add new global MCP \n",
      "server \n",
      "In the JSON Ô¨Åle, add what's shown below  \n",
      "80\n",
      "\n",
      "Context 3:\n",
      "DailyDoseofDS.com \n",
      "#5) Setup Crew and KickoÔ¨Ä \n",
      "We set up and kick oÔ¨Ä our Ô¨Ånancial analysis crew to get the result shown below! \n",
      " \n",
      "#6) Create MCP Server \n",
      "Now, we encapsulate our Ô¨Ånancial analyst within an MCP tool and add two more \n",
      "tools to enhance the user experience. \n",
      "‚óè save_code -> Saves generated code to local directory \n",
      "‚óè run_code_and_show_plot -> Executes the code and generates a plot \n",
      " \n",
      "57\n",
      "\n",
      "Question: What is MCP Server?\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "system_prompt_context = \"\"\"\n",
    "You are an AI assistant helping users find information in a document. Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "\"\"\"\n",
    "def create_prompt(user_query, context_chunks):\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "    prompt = f\"{system_prompt_context}\\n\\n{context_text}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What is MCP Server?\"\n",
    "    results = query_chromadb(user_query, top_k=3)\n",
    "    # print(f'Results Retrieved: {results}')\n",
    "    context_chunks = results['documents'][0]\n",
    "    # print(f'Context Chunks Retrieved: {len(context_chunks)}')\n",
    "    # print(f'First Context Chunk: {context_chunks[2]}...\\n\\n')\n",
    "    prompt = create_prompt(user_query, context_chunks)\n",
    "    print(\"Generated Prompt:\")\n",
    "    print(prompt)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991103d",
   "metadata": {},
   "source": [
    "### Call the Groq and OpenAI  LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f766b41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Initialize LLMs\n",
    "\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "google_llm = None\n",
    "google_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0, google_api_key=google_api_key)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0, groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63a8ad9",
   "metadata": {},
   "source": [
    "## Pass the query and context to create a response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05e2014b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Google LLM Response: The context describes a \"Multi-agent Deep Researcher\" that functions similarly to ChatGPT's deep research feature, providing detailed insights on any topic. It can be built as a local alternative using a specific tech stack. The workflow involves a user submitting a query, a web search agent performing a deep web search, a research analyst verifying and deduplicating the results, and a technical writer crafting a coherent response with citations. This system utilizes multiple agents working together, with one agent for research and another for analysis and writing.\n",
      "\n",
      "\n",
      " Groq LLM Response: Based on the provided context, a multi-agent team researcher appears to be a system where multiple agents work together to provide detailed insights on a topic. \n",
      "\n",
      "From Context 2, it seems that such a system can be built using a tech stack that includes a platform for deep web research, multi-agent orchestration, and a local server for deep research. The workflow involves:\n",
      "\n",
      "1. A user submitting a query\n",
      "2. A web search agent running a deep web search\n",
      "3. A research analyst verifying and deduplicating results\n",
      "4. A technical writer crafting a coherent response with citations\n",
      "\n",
      "This is similar to the concept described in Context 1, where multiple specialized agents collaborate to improve each other's outputs, such as one agent gathering data, another assessing risk, a third building strategy, and a fourth writing the report.\n",
      "\n",
      "Additionally, Context 3 mentions a Crew Setup with two Agents: an Analysis Agent and a Writer Agent, which also work together to analyze scraped content and produce insights.\n",
      "\n",
      "However, I don't have more specific information about the \"multi agent team researcher\" beyond what is described in these contexts.\n"
     ]
    }
   ],
   "source": [
    "def create_prompt(user_query, context_chunks):\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "    prompt = f\"{system_prompt_context}\\n\\n{context_text}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "def generate_openai_response(prompt):\n",
    "    response = openai_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Open AI Response: {response.content} *80')\n",
    "    return response.content\n",
    "\n",
    "def generate_google_response(prompt):\n",
    "    response = google_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Google LLM Response: {response.content}')\n",
    "    return response.content\n",
    "\n",
    "def generate_groq_response(prompt):\n",
    "    response = groq_llm.invoke(prompt)\n",
    "    print(f'\\n\\n Groq LLM Response: {response.content}')\n",
    "    return response.content\n",
    "\n",
    "\n",
    "\n",
    "user_query = \"Tell me more about multi agent team researcher?\"\n",
    "results = query_chromadb(user_query, top_k=3)\n",
    "\n",
    "# print(f'Results Retrieved: {results}')\n",
    "context_chunks = results['documents'][0]\n",
    "# print(f'Context Chunks Retrieved: {len(context_chunks)}')\n",
    "# print(f'First Context Chunk: {context_chunks[2]}...\\n\\n')\n",
    "prompt = create_prompt(user_query, context_chunks)\n",
    "# print(\"Generated Prompt:\")\n",
    "# print(prompt)  \n",
    "\n",
    "# Generate responses from both LLMs\n",
    "gemini_llmresponse = generate_google_response(prompt)\n",
    "groq_response = generate_groq_response(prompt)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a199d01",
   "metadata": {},
   "source": [
    "## Save the final Syntentized Response in sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d64e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Synthesized Final Response:\n",
      "A \"Multi-agent Deep Researcher\" or \"multi-agent team researcher\" is a system designed to provide detailed insights on any given topic, functioning analogously to advanced deep research features found in platforms like ChatGPT. This system is built upon the principle of **collaboration between multiple specialized agents**, each performing a distinct role to achieve a comprehensive and well-structured output.\n",
      "\n",
      "The core workflow of such a system typically involves the following steps:\n",
      "\n",
      "1.  **User Query Submission:** The process begins with a user submitting a specific query or topic of interest.\n",
      "2.  **Deep Web Search:** A dedicated **web search agent** is deployed to conduct an extensive search across the deep web, gathering a broad range of relevant information.\n",
      "3.  **Research Analysis and Verification:** A **research analyst agent** then takes over to meticulously verify the gathered information, deduplicate redundant findings, and ensure the accuracy and reliability of the data.\n",
      "4.  **Coherent Response Generation:** Finally, a **technical writer agent** synthesizes the verified and analyzed information into a coherent, well-structured response. This output is often enhanced with proper citations to attribute the sources of information.\n",
      "\n",
      "This multi-agent approach allows for a division of labor, where each agent focuses on its specific expertise. For instance, one agent might be responsible for data gathering, another for risk assessment or strategy building, and a third for report writing. This collaborative model, as seen in setups with distinct \"Analysis Agent\" and \"Writer Agent\" roles, ensures that the final output is not only informative but also critically assessed and professionally presented.\n",
      "\n",
      "Such a system can be constructed using a specific tech stack that includes platforms for deep web research, robust multi-agent orchestration capabilities, and a local server to facilitate deep research operations. The underlying concept is that by having specialized agents work in tandem, the overall research process is more efficient, thorough, and yields higher-quality insights than a single-agent approach.\n",
      "\n",
      "\n",
      " embedd query is 0.007471877615898848\n",
      "Fetched vector binding 0.007471877615898848\n",
      "\n",
      "\n",
      " Closing Database connection\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "\n",
    "synthesis_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Synthesize these two answers into one comprehensive response:\n",
    "\n",
    "Answer from Gemini LLM:\n",
    "{response1}\n",
    "\n",
    "Answer from Groq Llama:\n",
    "{response2}\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Instructions:\n",
    "1. Combine the best insights from both answers\n",
    "2. Avoid redundancy\n",
    "3. Provide a complete, well-organized answer\n",
    "4. Maintain accuracy and clarity\n",
    "\n",
    "Provide a single synthesized answer that is better and more complete than both.\n",
    "\"\"\")\n",
    "\n",
    "chain_synthesis = synthesis_prompt | google_llm | StrOutputParser()\n",
    "final_response = chain_synthesis.invoke({\n",
    "    \"response1\": gemini_llmresponse,\n",
    "    \"response2\": groq_response,\n",
    "    \"question\": user_query,\n",
    "\n",
    "})\n",
    "\n",
    "print(f\"‚úì Synthesized Final Response:\\n{final_response}\")\n",
    "\n",
    "# create a database connection\n",
    "sqlite_db_conn = sqlite3.connect(\"documents.db\")\n",
    "\n",
    "# Save it to SQLite\n",
    "cursor = sqlite_db_conn.cursor()\n",
    "\n",
    "# 2. Create a table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS llm_response (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    user_query TEXT,\n",
    "    vector_embedding BLOB,\n",
    "    llmresponse TEXT\n",
    ")\n",
    "\"\"\")\n",
    "# convert the query_vectorbinding into a BLOB using numpy array\n",
    "queryEmbedding = embeddingMModel.encode(user_query)\n",
    "print(f'\\n\\n embedd query is {queryEmbedding[0]}')\n",
    "\n",
    "embeddingBlob = np.array(queryEmbedding,dtype=np.float32).tobytes()\n",
    "\n",
    "\n",
    "# 3. Insert a record\n",
    "cursor.execute(\"\"\"\n",
    "INSERT INTO llm_response (user_query, vector_embedding, llmresponse)\n",
    "VALUES (?, ?, ?)\n",
    "\"\"\", (user_query, embeddingBlob, final_response))\n",
    "\n",
    "# 4. Commit changes\n",
    "sqlite_db_conn.commit()\n",
    "\n",
    "# 5. Verify vector embedding by decode\n",
    "cursor.execute(\n",
    "    \"SELECT vector_embedding FROM llm_response WHERE user_query = ?\",\n",
    "    (user_query,)\n",
    ")\n",
    "blob = cursor.fetchone()[0]\n",
    "\n",
    "vectorFetchedVal = np.frombuffer(blob, dtype=np.float32)\n",
    "print(f'Fetched vector binding {vectorFetchedVal[0]}')\n",
    "\n",
    "\n",
    "print(\"\\n\\n Closing Database connection\")\n",
    "sqlite_db_conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989de05",
   "metadata": {},
   "source": [
    "## Shorten the summary to 280 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "300cfc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "‚úì AI-Generated Summary (280 characters max):\n",
      "================================================================================\n",
      "A Multi-agent Deep Researcher uses specialized agents to collaborate on user queries. A search agent gathers info, an analyst verifies it, and a writer generates a coherent, cited response. This division of labor ensures efficient, thorough, and high-quality research insights.\n",
      "\n",
      "Original length: 2142 characters\n",
      "Summary length: 277 characters\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a summarization prompt to condense the response to 280 characters\n",
    "summarization_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize the following text in exactly 280 characters or less. \n",
    "Make it concise, clear, and informative. Preserve the key message.\n",
    "\n",
    "Text to summarize:\n",
    "{text}\n",
    "\n",
    "Important: Your summary must be exactly 280 characters or less (including spaces and punctuation).\n",
    "Respond with ONLY the summarized text, no additional explanation.\n",
    "\"\"\")\n",
    "\n",
    "# Use Google LLM to create the 280-character summary\n",
    "chain_summarize = summarization_prompt | google_llm | StrOutputParser()\n",
    "\n",
    "shortened_response = chain_summarize.invoke({\n",
    "    \"text\": final_response\n",
    "})\n",
    "\n",
    "# Ensure the response doesn't exceed 280 characters (fallback truncation)\n",
    "if len(shortened_response) > 280:\n",
    "    shortened_response = shortened_response[:277] + \"...\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì AI-Generated Summary (280 characters max):\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"{shortened_response}\")\n",
    "print(f\"\\nOriginal length: {len(final_response)} characters\")\n",
    "print(f\"Summary length: {len(shortened_response)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cd141",
   "metadata": {},
   "source": [
    "### Send the summarized summary as a email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c88df4ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìß Sending email via SendGrid...\n",
      "‚ùå Error sending email: HTTP Error 401: Unauthorized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sendgrid import SendGridAPIClient\n",
    "from sendgrid.helpers.mail import Mail\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.application import MIMEApplication\n",
    "import os\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "load_dotenv()\n",
    "\n",
    "def send_email_with_sendgrid(summarized_text, recipient_email=\"cloudwizkid1@gmail.com\"):\n",
    "    \"\"\"\n",
    "    Send the summarized response via email using SendGrid API\n",
    "    \"\"\"\n",
    "    # Get SendGrid API key from environment\n",
    "    sendgrid_api_key = os.getenv(\"SENDGRID_API_KEY\")\n",
    "    sender_email = \"cloudwizkid1@gmail.com\"\n",
    "    \n",
    "    if not sendgrid_api_key:\n",
    "        print(\"‚ùå Error: SENDGRID_API_KEY not found in .env file\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Create HTML email body\n",
    "        html_body = f\"\"\"\n",
    "        <html>\n",
    "          <body style=\"font-family: Arial, sans-serif; line-height: 1.6; color: #333;\">\n",
    "            <div style=\"max-width: 600px; margin: 0 auto; padding: 20px; border: 1px solid #ddd; border-radius: 5px;\">\n",
    "              <h2 style=\"color: #4CAF50;\">‚úì AI-Generated Summary</h2>\n",
    "              \n",
    "              <p><strong>Topic:</strong> Multi-Agent Team Researcher</p>\n",
    "              \n",
    "              <div style=\"background-color: #f5f5f5; padding: 15px; border-left: 4px solid #4CAF50; margin: 20px 0;\">\n",
    "                <p>{summarized_text}</p>\n",
    "              </div>\n",
    "              \n",
    "              <p><strong>Summary Statistics:</strong></p>\n",
    "              <ul>\n",
    "                <li>Character Length: {len(summarized_text)}/280</li>\n",
    "                <li>Generated using: Google Generative AI (Gemini)</li>\n",
    "                <li>Timestamp: {pd.Timestamp.now()}</li>\n",
    "              </ul>\n",
    "              \n",
    "              <hr>\n",
    "              <p style=\"font-size: 12px; color: #666;\">This is an automated email from AI Capstone Assignment. Please do not reply to this email.</p>\n",
    "            </div>\n",
    "          </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create plain text version\n",
    "        text_body = f\"\"\"\n",
    "        AI-Generated Summary\n",
    "        ====================\n",
    "        \n",
    "        Topic: Multi-Agent Team Researcher\n",
    "        \n",
    "        {summarized_text}\n",
    "        \n",
    "        Character Length: {len(summarized_text)}/280\n",
    "        Generated using: Google Generative AI (Gemini)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Send email via SendGrid API\n",
    "        print(\"\\nüìß Sending email via SendGrid...\")\n",
    "        message = Mail(\n",
    "            from_email=sender_email,\n",
    "            to_emails=recipient_email,\n",
    "            subject=\"AI-Generated Summary: Multi-Agent Team Researcher\",\n",
    "            plain_text_content=text_body,\n",
    "            html_content=html_body\n",
    "        )\n",
    "        \n",
    "        sg = SendGridAPIClient(sendgrid_api_key)\n",
    "        response = sg.send(message)\n",
    "        \n",
    "        print(f\"‚úì Email sent successfully to {recipient_email}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sending email: {e}\")\n",
    "        return False\n",
    "\n",
    "# Send the summarized response\n",
    "send_email_with_sendgrid(shortened_response,\"cloudwizkid1@gmail.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d52136",
   "metadata": {},
   "source": [
    "### Load and authenticate twilio configurations to send whatsapp and sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98cbcc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì SMS sent successfully to +918860811855. Message SID: SMa7118e0234c1614013d8f0050dacb340\n",
      "‚úì WhatsApp message sent successfully to +918860811855. Message SID: SM6888e817485bf8aad9e1f039d5b3507a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "twilio_account_sid = os.getenv(\"TWILIO_ACCOUNT_SID\")\n",
    "twilio_auth_token = os.getenv(\"TWILIO_AUTH_TOKEN\")\n",
    "twilio_phone_number = os.getenv(\"TWILIO_PHONE_NUMBER\")\n",
    "whatsapp_from = os.getenv(\"TWILIO_WHATSAPP_NUMBER\", \"whatsapp:+14155238886\")\n",
    "#print(f'Twilio SID: {twilio_account_sid}, Token: {twilio_auth_token}, Phone: {twilio_phone_number}')\n",
    "from twilio.rest import Client\n",
    "\n",
    "client = Client(twilio_account_sid, twilio_auth_token)\n",
    "\n",
    "def send_sms_via_twilio(to_phone_number, message_body):\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            body=message_body,\n",
    "            from_=twilio_phone_number,\n",
    "            to=to_phone_number\n",
    "        )\n",
    "        print(f\"‚úì SMS sent successfully to {to_phone_number}. Message SID: {message.sid}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sending SMS: {e}\")\n",
    "        return False\n",
    "# Send the summarized response via SMS\n",
    "send_sms_via_twilio(\"+918860811855\", shortened_response)\n",
    "\n",
    "# send whatsapp message via twilio\n",
    "\n",
    "def send_whatsapp_via_twilio(to_phone_number, message_body):\n",
    "    try:\n",
    "        message = client.messages.create(\n",
    "            body=message_body,\n",
    "            from_=whatsapp_from,\n",
    "            to='whatsapp:' + to_phone_number\n",
    "        )\n",
    "        print(f\"‚úì WhatsApp message sent successfully to {to_phone_number}. Message SID: {message.sid}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error sending WhatsApp message: {e}\")\n",
    "        return False\n",
    "# Send the summarized response via WhatsApp\n",
    "send_whatsapp_via_twilio(\"+918860811855\", shortened_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7a8c0",
   "metadata": {},
   "source": [
    "### Send the summarized response in a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0404b7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error: 403 Forbidden - your app may not have Write permissions\n",
      "Full Response: {\n",
      "  \"title\": \"Unsupported Authentication\",\n",
      "  \"detail\": \"Authenticating with OAuth 2.0 Application-Only is forbidden for this endpoint.  Supported authentication types are [OAuth 1.0a User Context, OAuth 2.0 User Context].\",\n",
      "  \"type\": \"https://api.twitter.com/2/problems/unsupported-authentication\",\n",
      "  \"status\": 403\n",
      "}\n",
      "\n",
      "To fix this:\n",
      "1. Go to https://developer.twitter.com/en/portal/dashboard\n",
      "2. Select your app and go to App Settings\n",
      "3. Under 'User authentication settings', ensure Permissions are set to 'Read and Write'\n",
      "4. Regenerate your tokens after changing permissions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# Get Bearer Token for Twitter API v2 and decode if URL-encoded\n",
    "twitter_bearer_token = os.getenv(\"TWITTER_BEARER_TOKEN\")\n",
    "if twitter_bearer_token:\n",
    "    twitter_bearer_token = unquote(twitter_bearer_token)\n",
    "\n",
    "def send_tweet(tweet_text):\n",
    "    \"\"\"\n",
    "    Send tweet using Twitter API v2 with Bearer Token (HTTP request)\n",
    "    \"\"\"\n",
    "    if not twitter_bearer_token:\n",
    "        print(\"‚ùå Error: TWITTER_BEARER_TOKEN not found in .env file\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Twitter API v2 endpoint\n",
    "        url = \"https://api.twitter.com/2/tweets\"\n",
    "        \n",
    "        # Prepare headers with Bearer Token\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {twitter_bearer_token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        # Prepare payload\n",
    "        payload = {\"text\": tweet_text}\n",
    "        \n",
    "        # Make POST request\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        \n",
    "        # Check if tweet was created successfully (201 = Created)\n",
    "        if response.status_code == 201:\n",
    "            data = response.json()\n",
    "            print(f\"‚úì Tweet posted successfully!\")\n",
    "            print(f\"Tweet ID: {data['data']['id']}\")\n",
    "            print(f\"Tweet text: {tweet_text[:100]}...\")\n",
    "            return True\n",
    "        elif response.status_code == 401:\n",
    "            print(\"‚ùå Error: Bearer Token is invalid or expired\")\n",
    "            print(\"Full Response:\", response.text)\n",
    "            return False\n",
    "        elif response.status_code == 403:\n",
    "            print(\"‚ùå Error: 403 Forbidden - your app may not have Write permissions\")\n",
    "            print(\"Full Response:\", response.text)\n",
    "            print(\"\\nTo fix this:\")\n",
    "            print(\"1. Go to https://developer.twitter.com/en/portal/dashboard\")\n",
    "            print(\"2. Select your app and go to App Settings\")\n",
    "            print(\"3. Under 'User authentication settings', ensure Permissions are set to 'Read and Write'\")\n",
    "            print(\"4. Regenerate your tokens after changing permissions\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"‚ùå Error: HTTP {response.status_code}\")\n",
    "            print(\"Full Response:\", response.text)\n",
    "            return False\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Send the summarized response via Tweet\n",
    "send_tweet(shortened_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
